{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "730a5ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████▌                                                                         | 1/8 [00:00<00:03,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed 300_TRANSCRIPT.csv → features saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|█████████████████████                                                               | 2/8 [00:00<00:02,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed 301_TRANSCRIPT.csv → features saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▌                                                    | 3/8 [00:00<00:01,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed 302_TRANSCRIPT.csv → features saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████                                          | 4/8 [00:01<00:01,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed 303_TRANSCRIPT.csv → features saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████████████████████████████▌                               | 5/8 [00:01<00:00,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed 304_TRANSCRIPT.csv → features saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████████                     | 6/8 [00:01<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed 319_TRANSCRIPT.csv → features saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████▌          | 7/8 [00:02<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed 320_TRANSCRIPT.csv → features saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:02<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed 321_TRANSCRIPT.csv → features saved\n",
      "\n",
      "All transcripts converted. Utterance‑level files (*.parquet) and session‑level JSON summaries are in the 'Processed' folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, re, string, json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 0) make sure NLTK data is downloaded once\n",
    "import nltk, ssl, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "try: nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "# ─── paths ──────────────────────────────────────────────────────────────\n",
    "BASE_DIR = Path.cwd()               # Audio Model\n",
    "IN_DIR   = BASE_DIR / \"Transcripts\" # holds 300_TRANSCRIPT.csv … \n",
    "OUT_DIR  = BASE_DIR / \"Processed\"   # where the output will be saved\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# helper: convert one row string → dict\n",
    "ROW_RGX = re.compile(\n",
    "    r'^\"?(?P<start>[\\d.]+)\\s+(?P<stop>[\\d.]+)\\s+(?P<speaker>\\w+)\\s+(?P<text>.*)\"?$'\n",
    ")\n",
    "def parse_row(row: str):\n",
    "    m = ROW_RGX.match(row.strip())\n",
    "    if not m: return None\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"start\"   : float(d[\"start\"]),\n",
    "        \"stop\"    : float(d[\"stop\"]),\n",
    "        \"speaker\" : d[\"speaker\"],\n",
    "        \"text\"    : d[\"text\"].strip()\n",
    "    }\n",
    "\n",
    "# helper: clean utterance text\n",
    "STOP = set(stopwords.words(\"english\"))\n",
    "FILLERS = {\"uh\", \"um\", \"mhm\", \"hmm\"}\n",
    "PUNCT  = set(string.punctuation)\n",
    "def clean_text(t):\n",
    "    t = re.sub(r\"\\[.*?\\]\", \" \", t)        # remove [laughter] etc.\n",
    "    words = [w.lower() for w in word_tokenize(t)\n",
    "             if w.lower() not in STOP and w.lower() not in FILLERS\n",
    "             and not all(ch in PUNCT for ch in w)]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# helper: feature extraction for one utterance\n",
    "def utterance_feats(text):\n",
    "    blob = TextBlob(text)\n",
    "    return {\n",
    "        \"len_char\"     : len(text),\n",
    "        \"len_words\"    : len(text.split()),\n",
    "        \"polarity\"     : blob.sentiment.polarity,\n",
    "        \"subjectivity\" : blob.sentiment.subjectivity,\n",
    "        \"noun_ratio\"   : _pos_ratio(text, \"NN\"),\n",
    "        \"verb_ratio\"   : _pos_ratio(text, \"VB\"),\n",
    "    }\n",
    "\n",
    "def _pos_ratio(txt, tag_prefix):\n",
    "    tags = pos_tag(word_tokenize(txt))\n",
    "    if not tags: return 0\n",
    "    return sum(1 for _, t in tags if t.startswith(tag_prefix)) / len(tags)\n",
    "\n",
    "# ─── iterate over all transcript files in \"Transcripts\" folder ─────────────────────────────────────────\n",
    "for fn in tqdm(sorted(IN_DIR.glob(\"*_TRANSCRIPT.csv\"))):\n",
    "    participant_id = fn.stem.split(\"_\")[0]   # Extract participant ID (e.g., \"300\")\n",
    "    with open(fn, encoding=\"utf8\") as f:\n",
    "        rows = [parse_row(r) for r in f if parse_row(r)]\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"text_clean\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "    # utterance‑level features\n",
    "    feats = pd.DataFrame(df[\"text_clean\"].apply(utterance_feats).tolist())\n",
    "    df = pd.concat([df, feats], axis=1)\n",
    "\n",
    "    # session‑level aggregates (mean, std)\n",
    "    agg = df[[\"polarity\", \"subjectivity\",\n",
    "              \"len_char\", \"len_words\",\n",
    "              \"noun_ratio\", \"verb_ratio\"]].agg([\"mean\",\"std\"]).unstack().to_dict()\n",
    "    agg = {f\"{k[0]}_{k[1]}\": v for k,v in agg.items()}  # flatten keys\n",
    "    agg[\"duration_total\"] = df[\"stop\"].iloc[-1] - df[\"start\"].iloc[0]\n",
    "    agg[\"participant\"]    = participant_id\n",
    "\n",
    "    # save utterance features\n",
    "    df.to_parquet(OUT_DIR / f\"{participant_id}_utterances.parquet\", index=False)\n",
    "    \n",
    "    # save session summary in JSON format\n",
    "    with open(OUT_DIR / f\"{participant_id}_transcript_features.json\", \"w\") as fp:\n",
    "        json.dump(agg, fp, indent=2)\n",
    "\n",
    "    print(f\"✓ Processed {fn.name} → features saved\")\n",
    "\n",
    "print(\"\\nAll transcripts converted. Utterance‑level files (*.parquet) and \"\n",
    "      \"session‑level JSON summaries are in the 'Processed' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7fd940d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>len_char</th>\n",
       "      <th>len_words</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>verb_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.428</td>\n",
       "      <td>35.888</td>\n",
       "      <td>Ellie</td>\n",
       "      <td>hi i'm ellie thanks for coming in today i was ...</td>\n",
       "      <td>hi 'm ellie thanks coming today created talk p...</td>\n",
       "      <td>75</td>\n",
       "      <td>12</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.738</td>\n",
       "      <td>33.068</td>\n",
       "      <td>Participant</td>\n",
       "      <td>thank you</td>\n",
       "      <td>thank</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.598</td>\n",
       "      <td>40.948</td>\n",
       "      <td>Ellie</td>\n",
       "      <td>think of me as a friend i don't judge i can't ...</td>\n",
       "      <td>think friend n't judge ca n't 'm computer</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.088</td>\n",
       "      <td>42.518</td>\n",
       "      <td>Participant</td>\n",
       "      <td>mmm k</td>\n",
       "      <td>mmm k</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42.358</td>\n",
       "      <td>51.738</td>\n",
       "      <td>Ellie</td>\n",
       "      <td>i'm here to learn about people and would love ...</td>\n",
       "      <td>'m learn people would love learn 'll ask quest...</td>\n",
       "      <td>125</td>\n",
       "      <td>20</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    start    stop      speaker  \\\n",
       "0  29.428  35.888        Ellie   \n",
       "1  32.738  33.068  Participant   \n",
       "2  36.598  40.948        Ellie   \n",
       "3  42.088  42.518  Participant   \n",
       "4  42.358  51.738        Ellie   \n",
       "\n",
       "                                                text  \\\n",
       "0  hi i'm ellie thanks for coming in today i was ...   \n",
       "1                                          thank you   \n",
       "2  think of me as a friend i don't judge i can't ...   \n",
       "3                                              mmm k   \n",
       "4  i'm here to learn about people and would love ...   \n",
       "\n",
       "                                          text_clean  len_char  len_words  \\\n",
       "0  hi 'm ellie thanks coming today created talk p...        75         12   \n",
       "1                                              thank         5          1   \n",
       "2          think friend n't judge ca n't 'm computer        41          8   \n",
       "3                                              mmm k         5          2   \n",
       "4  'm learn people would love learn 'll ask quest...       125         20   \n",
       "\n",
       "   polarity  subjectivity  noun_ratio  verb_ratio  \n",
       "0  0.366667      0.433333    0.583333        0.25  \n",
       "1  0.000000      0.000000    1.000000        0.00  \n",
       "2  0.000000      0.000000    0.375000        0.25  \n",
       "3  0.000000      0.000000    1.000000        0.00  \n",
       "4  0.300000      0.716667    0.350000        0.30  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# path to the file you saved earlier\n",
    "file_path = \"Processed/301_utterances.parquet\"\n",
    "\n",
    "# load into a DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# quick loo\n",
    "df.head()       # first 5 rows\n",
    "       # column dtypes & non‑null counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "befd50e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 174 entries, 0 to 173\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   start         174 non-null    float64\n",
      " 1   stop          174 non-null    float64\n",
      " 2   speaker       174 non-null    object \n",
      " 3   text          174 non-null    object \n",
      " 4   text_clean    174 non-null    object \n",
      " 5   len_char      174 non-null    int64  \n",
      " 6   len_words     174 non-null    int64  \n",
      " 7   polarity      174 non-null    float64\n",
      " 8   subjectivity  174 non-null    float64\n",
      " 9   noun_ratio    174 non-null    float64\n",
      " 10  verb_ratio    174 non-null    float64\n",
      "dtypes: float64(6), int64(2), object(3)\n",
      "memory usage: 15.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173db181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
